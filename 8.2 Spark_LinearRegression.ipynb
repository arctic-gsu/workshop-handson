{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWY-Yx5qS1UH",
    "outputId": "c8b2f0bb-6662-48b6-c577-95c6ccfaf33f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=6a1f33bb27a7fdf1f9b83908d34b8433c35dee4847dece852e9fefcb85de54ab\n",
      "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jjCtH149S1_w"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qvx-bvV5Tm75"
   },
   "source": [
    "Initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6hTEQ0LNTFYG"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"LinearRegression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHt7q6kNTpbk"
   },
   "source": [
    "Get input data\n",
    "\n",
    "Input data example:\n",
    "\n",
    "We have 2 columns seperated by comma.\n",
    "\n",
    "-1.74,1.66</br>\n",
    "1.24,-1.18</br>\n",
    "0.29,-0.40</br>\n",
    "-0.13,0.09</br>\n",
    "-0.39,0.38</br>\n",
    "-1.79,1.73</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "92WY0SdHTmTl"
   },
   "outputs": [],
   "source": [
    "inputLines = spark.sparkContext.textFile(\"./regression.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNvawoBcUwSv"
   },
   "source": [
    "We use RDD interface to parse the data out. We then map x, where x represents each row of RDD, extracts first column which is the label which we are predicting. First column is amount spent, then after that there are features that we are using. In our case we only have one feature, i.e. the page speed. We could create a dense vector consisting multiple features to multivariate linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xYzeVmTYRgK",
    "outputId": "0fe8a732-4f65-4c2c-fdc1-d7be987555ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-1.74,1.66', '1.24,-1.18']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputLines.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9l3VUvDKUivR"
   },
   "outputs": [],
   "source": [
    "data = inputLines.map(lambda x:x.split(\",\")).map(lambda x:(float(x[0]), Vectors.dense(float(x[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBsiM3CWV6Kp"
   },
   "source": [
    "We now create spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySmyTD_-Uv7x",
    "outputId": "7baaebc3-2ef9-40d5-f959-9deed4e92cac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=-1.74, features=DenseVector([1.66])),\n",
       " Row(label=1.24, features=DenseVector([-1.18]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    StructField(\"features\", VectorUDT(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SrKBIKzmZ6Ez"
   },
   "outputs": [],
   "source": [
    "trainTest = df.randomSplit([0.8,0.2])\n",
    "trainDF = trainTest[0]\n",
    "testDF = trainTest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOoNTALibMzW",
    "outputId": "d9e065e9-d4c8-4e78-ada0-eba111161ae7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4CW-ykxZm9N"
   },
   "source": [
    "Initializing Linear Regression with hyperparameters:\n",
    "\n",
    "\n",
    "- maxIter: Controls the number of iterations for the optimization algorithm.\n",
    "- regParam: Determines the amount of regularization to apply, helping to prevent overfitting.\n",
    "- elasticNetParam: Balances between L1 and L2 regularization to combine their strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rUpVTCjmXWYp"
   },
   "outputs": [],
   "source": [
    "spark_LR = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WqAD8R_GXUqN"
   },
   "outputs": [],
   "source": [
    "model = spark_LR.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3Vy0XLHaliN"
   },
   "source": [
    "Next, we predict with our test set. Cache the data for doing stuffs with the result dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kY5yjZo9aVow"
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(testDF).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05krkezmabN5",
    "outputId": "943ab3c8-b0af-461c-c720-fb386a77e115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+\n",
      "|label|features|         prediction|\n",
      "+-----+--------+-------------------+\n",
      "|-2.09|  [1.97]|-1.3895233295705949|\n",
      "| -2.0|  [2.02]|-1.4248063975333296|\n",
      "|-1.91|  [1.86]|-1.3119005800525783|\n",
      "|-1.77|  [1.66]|-1.1707683082016387|\n",
      "|-1.75|  [1.69]|-1.1919381489792797|\n",
      "|-1.64|  [1.84]|-1.2977873528674844|\n",
      "| -1.6|  [1.63]|-1.1495984674239978|\n",
      "|-1.58|  [1.45]|-1.0225794227581524|\n",
      "|-1.57|  [1.56]|-1.1002021722761692|\n",
      "|-1.48|  [1.38]|-0.9731831276103237|\n",
      "| -1.4|  [1.32]| -0.930843446055042|\n",
      "|-1.36|  [1.41]|-0.9943529683879646|\n",
      "|-1.26|  [1.17]|-0.8249942421668374|\n",
      "|-1.25|  [1.32]| -0.930843446055042|\n",
      "|-1.22|   [1.2]|-0.8461640829444783|\n",
      "|-1.11|  [1.23]|-0.8673339237221191|\n",
      "|-1.09|  [1.06]|-0.7473714926488209|\n",
      "|-1.09|  [1.18]|-0.8320508557593843|\n",
      "|-0.95|  [0.84]|-0.5921259936127875|\n",
      "|-0.91|  [1.03]|-0.7262016518711799|\n",
      "+-----+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "22G6J76-aeqp"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Q-WwToM5cKky"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
