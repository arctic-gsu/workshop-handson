{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark doesnot work in hemera, done in google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "\n",
    "- Big data: The data which is big that we cannot hold up in our RAM or no longer fits to a single machine. We then distribute data.\n",
    "- Distributed system: We have one machine machine known as master node, and data and calculation distributed to other computers.\n",
    "\n",
    "- In local system: \n",
    "    - we are limited to processing power in local machine.\n",
    "- In Distributed:\n",
    "    - we can leverage cores are processors of multiple machineconnected through a network\n",
    "    - scaling is easy, just keep adding machine to the network. It is fault tolerant.\n",
    "\n",
    "### Hadoop:\n",
    "- It is a way of distributing data into multiple machine. \n",
    "- Hadoop Distributed File System (HDFS):\n",
    "    - to distribute large dataset\n",
    "    - allows to work with large amount of data distributed over a distributed system.\n",
    "    - uses blocks of data of size 128 MB and replicated 3 times.\n",
    "\n",
    "- Map Reduce: \n",
    "    - to distribute computational task to distributed dataset\n",
    "    - allows computation across distributed dataset.\n",
    "    - is a way of splitting computation tasks to distributed set of files\n",
    "    - consists of Job Tracker and multiple Task trackers. The job tracker send code to run on the task tracker, where the task tracker allocates cpu and memory for the tasks and monitor tasks on worker nodes.\n",
    "\n",
    "<img src=\"Mapreduce.png\" alt=\"Decision Trees\" width=\"400\">\n",
    "Figure 1: map reduce\n",
    "\n",
    "\n",
    "### Spark:\n",
    "- Sparks improves concepts of using distributed system.\n",
    "- It is alternative to Map Reduce.\n",
    "- Spark doesnot require data to be stored in HDFS format like Mapreduce and can use data stored in Cassandra, AWS S3, HDFS.\n",
    "- Spark is much faster than Map reduce because it stores data in RAM after each transformation and if RAM overflows, then spills data to disk. In contrast, mapreduce always transfers data to disk  after each map reduce operation. \n",
    "- It has Resilient Distributed Dataset(RDD). It had distributed set of data, fault tolerance, can operate in parallel, and ability to use many data sources\n",
    "- Spark does lazy evaluation i.e. it only processes when an action is called. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAsQEk3dg0Xw",
    "outputId": "4ba9e6a3-a8f8-43b9-f263-952258a10d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from Pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOPWjMrQhZee"
   },
   "source": [
    "- SparkConf\n",
    "  - Configure spark applicattion setting parameters such as master URL, application name, resource allocation.\n",
    "  -Passed in spark context during initialization\n",
    "- SparkContext\n",
    "  - entrypoint for interacting with spark functions\n",
    "  - responsible for coordinating the execution of tasks in spark cluster\n",
    "  - connection between spark application and spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "x31X4PKygw4O"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Bj05A52Lgz3l"
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingApp\")\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS8xQtu0m8bo"
   },
   "source": [
    "Loads data from file. Textfile breaks input file line by line so every line corresponds to one value in RDD. sO 1st value in RDD will be first line of text, 2nd value will be 2nd line of text. so if ur file has 5 lines, then there will be 5 values in RDD representing each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TRrFprfTm7TD"
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"/content/textfile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqzc0UP7ohUo",
    "outputId": "c31f6151-6cdb-400c-d208-d56f12913f97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/textfile.txt MapPartitionsRDD[11] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NyHm9I1Xo3XU",
    "outputId": "eec3e6de-daf6-4a4c-a789-8ee327b4bb85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['156 23 4 5', '123 45 67 8', '122 34 67 8', '123 56 89 4', '456 45 12 3']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l35JX1xjo7l0"
   },
   "source": [
    "1. Using a map function to get the 3rd value from each line. We then assign the result to a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Ii12CWPJoigD"
   },
   "outputs": [],
   "source": [
    "rating = lines.map(lambda x:x.split()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK4utSojqKTv"
   },
   "source": [
    "Perform action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1M9KE3PYqH0n",
    "outputId": "ce99532c-bf70-46a1-cd21-9395144c69a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '67', '67', '89', '12']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdcmwuiHqLkO",
    "outputId": "34d4e8cb-aae1-4f56-976d-be7284fa18e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'4': 1, '67': 2, '89': 1, '12': 1})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.countByValue()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
